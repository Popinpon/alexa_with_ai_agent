"""
Smart Speaker Agent - ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹
"""
import logging
import os
import time
from typing import Dict, List, Any

from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_openai import AzureChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI

from shared.types import LLMProvider, AgentState, ConversationState
from shared.switchbot_manager import SwitchBotManager
from shared.conversation_manager import ConversationManager
from shared.workflow_builder import WorkflowBuilder
from shared.gemini_agent import GeminiAgent

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)


class SmartSpeakerAgent:
    """ã‚¹ãƒãƒ¼ãƒˆã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œï¼‰"""
    
    def __init__(self, llm_provider: str = "azure_openai"):
        init_start = time.time()
        logger.info(f"ğŸš€ SmartSpeakerAgent initialization started")
        
        self.llm_provider = llm_provider
        
        # LLMä½œæˆæ™‚é–“è¨ˆæ¸¬
        llm_start = time.time()
        self.llm = self._create_llm()
        llm_time = time.time() - llm_start
        logger.info(f"â±ï¸ LLM creation: {llm_time:.3f}s")
        
        # GeminiAgentä½œæˆæ™‚é–“è¨ˆæ¸¬
        gemini_start = time.time()
        self.gemini_search_agent = GeminiAgent()
        gemini_time = time.time() - gemini_start
        logger.info(f"â±ï¸ GeminiAgent creation: {gemini_time:.3f}s")
        
        # ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        self.switchbot_manager = SwitchBotManager()
        self.conversation_manager = ConversationManager(self.llm)
        
        # ãƒ„ãƒ¼ãƒ«ã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ§‹ç¯‰
        self.tools = self._create_tools()
        self.device_ids = self.switchbot_manager.get_actual_device_ids()
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®æ§‹ç¯‰
        self.workflow_builder = WorkflowBuilder(self.llm, self.tools, self.gemini_search_agent)
        self.graph = self.workflow_builder.create_agent_graph()
        self.conversation_graph = self.workflow_builder.create_conversation_graph()
        
        self._initialized = True
        
        init_total = time.time() - init_start
        logger.info(f"âœ… SmartSpeakerAgent initialization completed: {init_total:.3f}s")
    
    def _create_llm(self):
        """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ã¦LLMã‚’ä½œæˆ"""
        if self.llm_provider == LLMProvider.AZURE_OPENAI.value:
            return AzureChatOpenAI(
                api_version=os.getenv("AZURE_OPENAI_VERSION"),
                azure_endpoint=os.getenv("AZURE_ENDPOINT"),
                api_key=os.getenv("AZURE_OPENAI_KEY"),
                azure_deployment=os.getenv("DEPLOYMENT_NAME"),
                temperature=0.1,
            )
        elif self.llm_provider == LLMProvider.GEMINI.value:
            return ChatGoogleGenerativeAI(
                model="gemini-2.5-flash-light",
                api_key=os.getenv("GEMINI_API_KEY"),
                temperature=0.1,
            )
        else:
            raise ValueError(f"Unsupported LLM provider: {self.llm_provider}")
    
    def _create_tools(self):
        """ãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆ"""
        create_tools_start = time.time()
        logger.info(f"ğŸ”§ Tool creation started")
        
        # SwitchBotãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆ
        switchbot_tools = self.switchbot_manager.create_switchbot_tools()
        
        # Geminiæ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆ
        gemini_tool_start = time.time()
        from langchain_core.tools import tool
        
        @tool
        def gemini_search(query: str) -> Dict[str, Any]:
            """Geminiã®æ¤œç´¢æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦Webæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™
            
            Args:
                query: èƒŒæ™¯ã¨æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆè³ªå•ã‚„èª¿ã¹ãŸã„ã“ã¨ï¼‰èƒŒæ™¯ã‚’å«ã‚ã‚‹ã¨æ¤œç´¢ç²¾åº¦ã‚‚å‘ä¸Š
            
            Returns:
                æ¤œç´¢çµæœã‚’å«ã‚€è¾æ›¸
            """
            try:
                result = self.gemini_search_agent.chat(query)
                
                return {
                    "response": result.response,
                    "citations": result.citations,
                    "grounding_metadata": result.grounding_metadata,
                    "success": True
                }
            except Exception as e:
                logger.error(f"Geminiæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                return {
                    "response": f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                    "citations": [],
                    "grounding_metadata": None,
                    "success": False
                }
        
        gemini_tool_time = time.time() - gemini_tool_start
        logger.info(f"â±ï¸ Gemini tool creation: {gemini_tool_time:.3f}s")
        
        # ãƒ„ãƒ¼ãƒ«ã‚’çµåˆ
        tools = switchbot_tools.copy()
        tools.append(gemini_search)
        
        # å…¨ä½“ã®æ™‚é–“è¨ˆæ¸¬
        create_tools_total = time.time() - create_tools_start
        logger.info(f"âœ… Tool creation completed: {len(tools)} tools in {create_tools_total:.3f}s")
        return tools
    
    def get_system_message(self) -> str:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™"""
        return """
ã‚ãªãŸã¯ã‚¹ãƒãƒ¼ãƒˆã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã®éŸ³å£°ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ï¼š
1. ç…§æ˜ã®ç‚¹ç¯ãƒ»æ¶ˆç¯åˆ¶å¾¡
2. ã‚¨ã‚¢ã‚³ãƒ³ã®æ¸©åº¦ãƒ»ãƒ¢ãƒ¼ãƒ‰ãƒ»é¢¨é‡åˆ¶å¾¡
3. å®¤å†…ç’°å¢ƒæƒ…å ±ã®ç¢ºèªï¼ˆæ¸©åº¦ã€æ¹¿åº¦ã€ç…§åº¦ï¼‰
4. Webæ¤œç´¢ã«ã‚ˆã‚‹æœ€æ–°æƒ…å ±ã®æä¾›

éŸ³å£°ã§ã®å¯¾è©±ã‚’å‰æã¨ã—ãŸå¿œç­”ã‚’ã—ã¦ãã ã•ã„ï¼š
- ç°¡æ½”ã§èãå–ã‚Šã‚„ã™ã„å›ç­”
- å°‚é–€ç”¨èªã¯é¿ã‘ã€åˆ†ã‹ã‚Šã‚„ã™ã„è¡¨ç¾ã‚’ä½¿ç”¨
- å›ç­”ã¯2-3æ–‡ç¨‹åº¦ã«åã‚ã‚‹
- æ•°å­—ã‚„æ™‚é–“ã¯æ˜ç¢ºã«èª­ã¿ä¸Šã’ã‚‹
ä¾‹ï¼š
- ã€Œé›»æ°—ã‚’ã¤ã‘ã¦ã€â†’ç…§æ˜ã‚’ç‚¹ç¯
- ã€Œæš‘ã„/å¯’ã„ã€â†’é©åˆ‡ãªæ¸©åº¦è¨­å®šã§ã‚¨ã‚¢ã‚³ãƒ³ã‚’æ“ä½œ
- ã€Œä»Šã®éƒ¨å±‹ã®æ¸©åº¦ã¯ï¼Ÿã€â†’ç’°å¢ƒæƒ…å ±ã‚’å–å¾—
- ã€Œã€œã«ã¤ã„ã¦æ•™ãˆã¦ã€â†’Webæ¤œç´¢ã§æœ€æ–°æƒ…å ±ã‚’æä¾›ã€‚è‡ªèº«ã®æƒ…å ±ã«ãªã„å ´åˆã¯å¿…ãšæ¤œç´¢ã™ã‚‹ã“ã¨

ã™ã¹ã¦ã®å¿œç­”ã¯éŸ³å£°ã§ã®èª­ã¿ä¸Šã’ã«é©ã—ãŸè‡ªç„¶ãªæ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚
"""
    
    async def chat_cycle(self, user_input: str, session_id: str) -> Dict[str, Any]:
        """ä¼šè©±ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œï¼ˆAlexaãƒ¬ã‚¤ãƒ¤ãƒ¼ç”¨ï¼‰- LangGraphãƒ™ãƒ¼ã‚¹"""
        # åˆæœŸçŠ¶æ…‹æ§‹ç¯‰
        conversation_history = {}
        if session_id not in conversation_history:
            conversation_history[session_id] = [SystemMessage(content=self.get_system_message())]
        
        messages = conversation_history[session_id].copy()
        messages.append(HumanMessage(content=user_input))
        
        initial_state: ConversationState = {
            "session_id": session_id,
            "messages": messages,
            "device_ids": self.device_ids,
            "llm_provider": self.llm_provider,
            "processing_start_time": time.time(),
            "current_task": None,
            "is_processing": False,
            "has_timeout": False,
            "partial_result": None,
            "user_input_type": "",
            "should_continue_processing": False,
            "prepared_response": "",
            "cycle_complete": False
        }
        
        try:
            # ä¼šè©±ã‚µã‚¤ã‚¯ãƒ«å°‚ç”¨ã®LangGraphã‚’å®Ÿè¡Œ
            result = await self.conversation_graph.ainvoke(initial_state)
            
            return {
                "prepared_response": result.get("prepared_response", "å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚"),
                "should_continue_processing": result.get("should_continue_processing", False)
            }
            
        except Exception as e:
            logger.error(f"Chat cycle error: {e}")
            return {
                "prepared_response": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                "should_continue_processing": False
            }

    async def chat(self, user_input: str, session_id: str, conversation_history: Dict[str, List] = None) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’å‡¦ç†ã—ã¦å¿œç­”ã‚’è¿”ã™"""
        start_time = time.time()
        
        try:
            if conversation_history is None:
                conversation_history = {}
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—ã¾ãŸã¯åˆæœŸåŒ–
            if session_id not in conversation_history:
                conversation_history[session_id] = [SystemMessage(content=self.get_system_message())]
            
            messages = conversation_history[session_id].copy()
            messages.append(HumanMessage(content=user_input))
            
            # åˆæœŸçŠ¶æ…‹ã‚’è¨­å®š
            state = AgentState(
                messages=messages,
                device_ids=self.device_ids,
                llm_provider=self.llm_provider
            )
            
            # ã‚°ãƒ©ãƒ•ã‚’éåŒæœŸå®Ÿè¡Œ
            graph_start = time.time()
            result = await self.graph.ainvoke(state)
            graph_time = time.time() - graph_start
            
            # ä¼šè©±å±¥æ­´ã‚’æ›´æ–°
            conversation_history[session_id] = result["messages"]
            
            # æœ€å¾Œã®AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
            last_message = result["messages"][-1]
            if isinstance(last_message, AIMessage):
                response_content = last_message.content
                
                # å®Ÿè¡Œæ™‚é–“ã®ãƒ­ã‚°å‡ºåŠ›
                total_time = time.time() - start_time
                logger.info(f"â±ï¸ Performance Metrics - Total: {total_time:.2f}s | Graph: {graph_time:.2f}s")
                logger.info(f"SmartSpeaker-Response: {response_content}")
                return response_content
            else:
                total_time = time.time() - start_time
                logger.warning(f"â±ï¸ Performance Metrics - Total: {total_time:.2f}s (Failed)")
                return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
                
        except Exception as e:
            total_time = time.time() - start_time
            error_message = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            logger.error(f"â±ï¸ Performance Metrics - Total: {total_time:.2f}s (Error)")
            logger.error(error_message)
            return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"




# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
async def main():
    """éåŒæœŸãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    try:
        agent = SmartSpeakerAgent("azure_openai")
        
        # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
        queries = [
            "ã‚¨ã‚¢ã‚³ãƒ³æ¶ˆã—ã¦",
            "ä»Šæ—¥ã®å¤©æ°—ã¯ï¼Ÿ"
        ]
        
        session_id = "test_session"
        
        for query in queries:
            print(f"\n=== è³ªå•: {query} ===")
            response = await agent.chat(query, session_id)
            print(f"å›ç­”: {response}")
            
    except Exception as e:
        print(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
        print("ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())